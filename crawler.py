import time
import os
import pandas as pd
import subprocess 
from datetime import datetime, timedelta

# [ë„¤ì´ë²„ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬]
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.keys import Keys

# [ìœ íŠœë¸Œìš© ë¼ì´ë¸ŒëŸ¬ë¦¬]
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# ======================================================
# [ì„¤ì • 1] ê³µí†µ ë° íŒŒì¼ ì„¤ì •
# ======================================================
FILE_NAME = "electlink_voc.csv"

# ======================================================
# [ì„¤ì • 2] ë„¤ì´ë²„ ì¹´í˜ ì„¤ì •
# ======================================================
NAVER_SEARCH_KEYWORDS = ["ì¼ë ‰ë§í¬", "ì›Œí„°", "ì±„ë¹„", "ì´ë¸Œì´ì‹œìŠ¤"] 
DEEP_SEARCH_KEYWORDS = ["ì›Œí„°", "ì±„ë¹„"] 
EXCLUDE_WORDS = ["íŒë‹ˆë‹¤", "ì‚½ë‹ˆë‹¤", "ë§¤ì…", "í¬ë ˆë”§", "ì–‘ë„", "ì¿ í°", "íŒë§¤", "êµ¬ë§¤"]
TARGET_CAFE_KEYWORDS = ["í…ŒìŠ¬ë¼", "ì „ê¸°ì°¨", "EV", "ì•„ì´ì˜¤ë‹‰"] 

# ======================================================
# [ì„¤ì • 3] ìœ íŠœë¸Œ ì„¤ì •
# ======================================================
YOUTUBE_API_KEY = os.environ.get("YOUTUBE_API_KEY") 
YOUTUBE_SEARCH_TOPICS = ["ì „ê¸°ì°¨ ì¶©ì „", "ê³ ì†ë„ë¡œ ì¶©ì „", "ì „ê¸°ì°¨ ìš”ê¸ˆ", "ê¸‰ì† ì¶©ì „", "íœ´ê²Œì†Œ ì¶©ì „"]
TARGET_BRANDS = ["SKì¼ë ‰ë§í¬", "ì¼ë ‰ë§í¬", "ì—ìŠ¤ì—ìŠ¤ì°¨ì €", "SSì°¨ì €", "ì›Œí„°", "ì±„ë¹„", "ì´ë¸Œì´ì‹œìŠ¤"]

# ======================================================
# [ê¸°ëŠ¥ 1] ìœ íŠœë¸Œ í¬ë¡¤ë§ í•¨ìˆ˜
# ======================================================
def crawl_youtube():
    print(f"\nğŸ“º [YouTube] í¬ë¡¤ë§ ì‹œì‘ (ì¡°íšŒìˆ˜ 10íšŒâ†‘, ë¸Œëœë“œ ê°•ì¡°)...")
    results = []
    
    if not YOUTUBE_API_KEY:
        print("âš ï¸ [YouTube] API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. (GitHub Secrets í™•ì¸ í•„ìš”)")
        return []

    try:
        youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)
        
        # 24ì‹œê°„ ì´ë‚´
        search_date = datetime.utcnow() - timedelta(days=1) 
        published_after = search_date.strftime("%Y-%m-%dT%H:%M:%SZ")
        query = "|".join(YOUTUBE_SEARCH_TOPICS)

        # 1. ì˜ìƒ ê²€ìƒ‰
        search_response = youtube.search().list(
            q=query, part="id", order="date",
            publishedAfter=published_after, type="video",
            maxResults=30 
        ).execute()

        video_ids = []
        seen_ids = set()
        for item in search_response.get("items", []):
            vid_id = item['id']['videoId']
            if vid_id not in seen_ids:
                video_ids.append(vid_id)
                seen_ids.add(vid_id)

        if not video_ids:
            print("   ğŸ’¨ ìµœê·¼ 24ì‹œê°„ ë‚´ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return []

        # 2. ìƒì„¸ ì •ë³´ ì¡°íšŒ
        video_response = youtube.videos().list(
            id=','.join(video_ids), part='snippet,statistics'
        ).execute()

        items = video_response.get("items", [])
        items.sort(key=lambda x: int(x['statistics'].get('viewCount', 0)), reverse=True)

        print(f"   ğŸ” 1ì°¨ ê²€ìƒ‰ëœ ì˜ìƒ {len(items)}ê°œ ë¶„ì„ ì¤‘...")

        for item in items:
            vid_id = item['id']
            stats = item['statistics']
            snippet = item['snippet']
            
            view_count = int(stats.get('viewCount', 0))
            if view_count < 10: continue

            raw_title = snippet['title']
            channel = snippet['channelTitle']

            title_display = raw_title
            brand_detected = False
            for brand in TARGET_BRANDS:
                if brand in raw_title:
                    title_display = title_display.replace(brand, f"*{brand}*")
                    brand_detected = True
            
            # [ë‚ ì§œ] í•œêµ­ ì‹œê°„ ê¸°ì¤€ ì ìš©
            kst_now = datetime.now() + timedelta(hours=9)
            date_str = kst_now.strftime("%Y-%m-%d") + " (New)"

            results.append({
                "ì‘ì„±ì¼": date_str,
                "í‚¤ì›Œë“œ": "ìœ íŠœë¸Œ(ì˜ìƒ)",
                "ì¹´í˜ëª…": f"[YouTube] {channel}",
                "ì œëª©": f"[ì˜ìƒ] {title_display} (ì¡°íšŒìˆ˜ {view_count}íšŒ)",
                "ë§í¬": f"https://www.youtube.com/watch?v={vid_id}",
                "ìˆ˜ì§‘ì‹œì ": kst_now.strftime("%Y-%m-%d %H:%M")
            })

            # 3. ëŒ“ê¸€ ìˆ˜ì§‘
            try:
                comment_response = youtube.commentThreads().list(
                    videoId=vid_id, part="snippet", textFormat="plainText", maxResults=5
                ).execute()
                
                for c_item in comment_response.get("items", []):
                    comment = c_item['snippet']['topLevelComment']['snippet']
                    text = comment['textDisplay'].replace('\n', ' ').strip()
                    author = comment['authorDisplayName']

                    found_brand_in_comment = False
                    for brand in TARGET_BRANDS:
                        if brand in text:
                            text = text.replace(brand, f"*{brand}*")
                            found_brand_in_comment = True
                    
                    if found_brand_in_comment:
                        if len(text) > 80: text = text[:80] + "..."
                        results.append({
                            "ì‘ì„±ì¼": date_str,
                            "í‚¤ì›Œë“œ": "ìœ íŠœë¸Œ(ëŒ“ê¸€)",
                            "ì¹´í˜ëª…": f"[YouTube] {author}",
                            "ì œëª©": f"ğŸ’¬ {text}",
                            "ë§í¬": f"https://www.youtube.com/watch?v={vid_id}",
                            "ìˆ˜ì§‘ì‹œì ": kst_now.strftime("%Y-%m-%d %H:%M")
                        })
            except HttpError: pass 

    except Exception as e:
        print(f"âŒ [YouTube] ì—ëŸ¬ ë°œìƒ: {e}")
    
    print(f"   âœ… ìœ íŠœë¸Œ ë°ì´í„° {len(results)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
    return results

# ======================================================
# [ê¸°ëŠ¥ 2] ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ í•¨ìˆ˜
# ======================================================
def crawl_naver():
    print(f"\nğŸš€ [Naver] ê³ ì„±ëŠ¥ í¬ë¡¤ëŸ¬ ì‹œì‘")
    data_list = []

    options = webdriver.ChromeOptions()
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    options.add_argument("--headless") 
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    time.sleep(2)

    try:
        for keyword in NAVER_SEARCH_KEYWORDS:
            print(f"   ğŸ” '{keyword}' ê²€ìƒ‰ ì¤‘...")
            base_url = "https://search.naver.com/search.naver?ssc=tab.cafe.all&st=date&nso=so%3Add%2Cp%3Aall&query="
            driver.get(base_url + keyword)
            time.sleep(3) 

            if keyword in DEEP_SEARCH_KEYWORDS:
                print(f"      ğŸ‘‰ ì‹¬ì¸µ ê²€ìƒ‰ ì§„í–‰ (15 pg)")
                scroll_times = 15 
            else:
                scroll_times = 3 

            body = driver.find_element(By.TAG_NAME, "body")
            for i in range(scroll_times):
                body.send_keys(Keys.END)
                time.sleep(1.2)
            time.sleep(2)

            articles = driver.find_elements(By.CSS_SELECTOR, "li.bx")
            keyword_count = 0
            
            for article in articles:
                try:
                    cafe_name = ""
                    try: cafe_name = article.find_element(By.CSS_SELECTOR, "a.txt_name").text
                    except: 
                        try: cafe_name = article.find_element(By.CSS_SELECTOR, "a.name").text
                        except: pass

                    if not any(target in cafe_name for target in TARGET_CAFE_KEYWORDS): continue
                    if not any(x in article.text for x in ["ë¶„ ì „", "ì‹œê°„ ì „", "ë°©ê¸ˆ ì „"]): continue
                    
                    try:
                        title_ele = article.find_element(By.CSS_SELECTOR, "a.title_link")
                        title = title_ele.text
                        link = title_ele.get_attribute("href")
                    except: continue

                    if any(bad_word in title for bad_word in EXCLUDE_WORDS): continue

                    # [ë‚ ì§œ] í•œêµ­ ì‹œê°„ ê¸°ì¤€ ì ìš©
                    kst_now = datetime.now() + timedelta(hours=9)
                    date_str = kst_now.strftime("%Y-%m-%d") + " (New)"

                    data_list.append({
                        "ì‘ì„±ì¼": date_str,
                        "í‚¤ì›Œë“œ": keyword, 
                        "ì¹´í˜ëª…": cafe_name,
                        "ì œëª©": title,
                        "ë§í¬": link,
                        "ìˆ˜ì§‘ì‹œì ": kst_now.strftime("%Y-%m-%d %H:%M")
                    })
                    keyword_count += 1
                except Exception: continue
            
            print(f"      âœ¨ ìˆ˜ì§‘: {keyword_count}ê±´")
            time.sleep(1)

    except Exception as e:
        print(f"ì—ëŸ¬ ë°œìƒ: {e}")
    finally:
        driver.quit()
    
    return data_list

# ======================================================
# [ê¸°ëŠ¥ 3] ë©”ì¸ ì‹¤í–‰ ë° ì €ì¥ (ì´ˆê¸°í™” ë¡œì§ ì ìš© âœ¨)
# ======================================================
if __name__ == "__main__":
    # 1. ìˆ˜ì§‘
    naver_data = crawl_naver()
    youtube_data = crawl_youtube()
    
    # 2. í•©ì¹˜ê¸°
    all_data = naver_data + youtube_data
    
    # 3. ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥
    df_new = pd.DataFrame(all_data)
    if not df_new.empty:
        df_new = df_new[["ì‘ì„±ì¼", "í‚¤ì›Œë“œ", "ì¹´í˜ëª…", "ì œëª©", "ë§í¬", "ìˆ˜ì§‘ì‹œì "]]

    if os.path.exists(FILE_NAME):
        try:
            df_old = pd.read_csv(FILE_NAME)
            
            # âœ… [í•µì‹¬ ìˆ˜ì •] ê¸°ì¡´ ë°ì´í„°ì—ì„œ '(New)' ê¼¬ë¦¬í‘œ ë–¼ê¸°! (ì´ˆê¸°í™”)
            if 'ì‘ì„±ì¼' in df_old.columns:
                df_old['ì‘ì„±ì¼'] = df_old['ì‘ì„±ì¼'].str.replace(" (New)", "", regex=False)
            
            # êµ¬ë²„ì „ íŒŒì¼ í˜¸í™˜ì„± ì²˜ë¦¬
            if "í‚¤ì›Œë“œ" not in df_old.columns:
                # íŒŒì¼ í¬ë§·ì´ ë‹¤ë¥´ë©´ ê·¸ëƒ¥ ë®ì–´ì”€
                if not df_new.empty:
                    df_new.to_csv(FILE_NAME, mode='w', header=True, index=False, encoding="utf-8-sig")
            else:
                # ì¤‘ë³µ ì œê±° í›„ ë³‘í•©
                if not df_new.empty:
                    existing_links = df_old['ë§í¬'].tolist()
                    df_final = df_new[~df_new['ë§í¬'].isin(existing_links)]
                else:
                    df_final = pd.DataFrame()

                # âœ… [í•µì‹¬ ìˆ˜ì •] Old(íƒœê·¸ ë—Œ) + New(íƒœê·¸ ìˆìŒ) í•©ì¹˜ê¸°
                combined_df = pd.concat([df_old, df_final], ignore_index=True)
                
                # âœ… [í•µì‹¬ ìˆ˜ì •] mode='w'ë¡œ ì „ì²´ ë®ì–´ì“°ê¸° (ê·¸ë˜ì•¼ íƒœê·¸ ë—€ ê²Œ ë°˜ì˜ë¨)
                combined_df.to_csv(FILE_NAME, index=False, encoding="utf-8-sig")
                
                if not df_final.empty:
                    print(f"\nğŸ’¾ ë¡œì»¬ ì €ì¥ ë° ê°±ì‹  ì™„ë£Œ ({len(df_final)}ê±´ ì‹ ê·œ ì¶”ê°€)")
                else:
                    print("\nğŸ’¾ ë¡œì»¬ ë°ì´í„° ê°±ì‹  ì™„ë£Œ (ì‹ ê·œ ë°ì´í„° ì—†ìŒ, íƒœê·¸ë§Œ ì •ë¦¬ë¨)")

                # GitHub ì—…ë¡œë“œ (íŒŒì¼ ë‚´ìš©ì´ ë°”ë€Œì—ˆìœ¼ë¯€ë¡œ ë¬´ì¡°ê±´ ì‹œë„)
                print("\nğŸ™ GitHub ì—…ë¡œë“œ ì‹œì‘...")
                subprocess.run(["git", "config", "--global", "user.name", "GitHub Action Bot"], check=False)
                subprocess.run(["git", "config", "--global", "user.email", "actions@github.com"], check=False)
                subprocess.run(["git", "add", FILE_NAME], check=True)
                commit_msg = f"Update data: {datetime.now().strftime('%Y-%m-%d %H:%M')}"
                try:
                    subprocess.run(["git", "commit", "-m", commit_msg], check=True)
                    subprocess.run(["git", "push"], check=True)
                    print("âœ… GitHub Push ì™„ë£Œ!")
                except subprocess.CalledProcessError:
                    print("   -> (GitHub) ë³€ê²½ ì‚¬í•­ ì—†ìŒ (ì´ë¯¸ ìµœì‹ )")

        except Exception as e:
            print(f"íŒŒì¼ ì²˜ë¦¬ ì—ëŸ¬: {e}")
            if not df_new.empty:
                df_new.to_csv(FILE_NAME, mode='w', header=True, index=False, encoding="utf-8-sig")
    else:
        # íŒŒì¼ì´ ì—†ì„ ë•Œ
        if not df_new.empty:
            df_new.to_csv(FILE_NAME, mode='w', header=True, index=False, encoding="utf-8-sig")
            print(f"\nâœ… ì‹ ê·œ íŒŒì¼ ìƒì„± ì™„ë£Œ.")
            subprocess.run(["git", "add", FILE_NAME], check=True)
            subprocess.run(["git", "commit", "-m", "Init data"], check=True)
            subprocess.run(["git", "push"], check=True)
        else:
            print("\nğŸ’¤ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
