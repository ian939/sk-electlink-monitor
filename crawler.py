import time
import os
import pandas as pd
import subprocess 
from datetime import datetime, timedelta

# [ë„¤ì´ë²„ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬]
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.keys import Keys

# [ìœ íŠœë¸Œìš© ë¼ì´ë¸ŒëŸ¬ë¦¬]
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# ======================================================
# [ì„¤ì • 1] ê³µí†µ ë° íŒŒì¼ ì„¤ì •
# ======================================================
FILE_NAME = "electlink_voc.csv"

# ======================================================
# [ì„¤ì • 2] ë„¤ì´ë²„ ì¹´í˜ ì„¤ì •
# ======================================================
NAVER_SEARCH_KEYWORDS = ["ì¼ë ‰ë§í¬", "ì›Œí„°", "ì±„ë¹„", "ì´ë¸Œì´ì‹œìŠ¤"] 
DEEP_SEARCH_KEYWORDS = ["ì›Œí„°", "ì±„ë¹„"] # ì‹¬ì¸µ ê²€ìƒ‰ í‚¤ì›Œë“œ
EXCLUDE_WORDS = ["íŒë‹ˆë‹¤", "ì‚½ë‹ˆë‹¤", "ë§¤ì…", "í¬ë ˆë”§", "ì–‘ë„", "ì¿ í°", "íŒë§¤", "êµ¬ë§¤"]
TARGET_CAFE_KEYWORDS = ["í…ŒìŠ¬ë¼", "ì „ê¸°ì°¨", "EV", "ì•„ì´ì˜¤ë‹‰"] 

# ======================================================
# [ì„¤ì • 3] ìœ íŠœë¸Œ ì„¤ì • (í…ŒìŠ¤íŠ¸ ì™„ë£Œëœ ë¡œì§ ì ìš©)
# ======================================================
YOUTUBE_API_KEY = os.environ.get("YOUTUBE_API_KEY") 
# ë„“ê²Œ ê²€ìƒ‰í•  ì£¼ì œ
YOUTUBE_SEARCH_TOPICS = ["ì „ê¸°ì°¨ ì¶©ì „", "ê³ ì†ë„ë¡œ ì¶©ì „", "ì „ê¸°ì°¨ ìš”ê¸ˆ", "ê¸‰ì† ì¶©ì „", "íœ´ê²Œì†Œ ì¶©ì „"]
# ê°•ì¡°í•  ë¸Œëœë“œ (ë³¼ë“œ ì²˜ë¦¬)
TARGET_BRANDS = ["SKì¼ë ‰ë§í¬", "ì¼ë ‰ë§í¬", "ì—ìŠ¤ì—ìŠ¤ì°¨ì €", "SSì°¨ì €", "ì›Œí„°", "ì±„ë¹„", "ì´ë¸Œì´ì‹œìŠ¤"]

# ======================================================
# [ê¸°ëŠ¥ 1] ìœ íŠœë¸Œ í¬ë¡¤ë§ í•¨ìˆ˜ (ì¡°íšŒìˆ˜ 10íšŒ ì´ìƒ, ë¸Œëœë“œ í•„í„°)
# ======================================================
def crawl_youtube():
    print(f"\nğŸ“º [YouTube] í¬ë¡¤ë§ ì‹œì‘ (ì¡°íšŒìˆ˜ 10íšŒâ†‘, ë¸Œëœë“œ ê°•ì¡°)...")
    results = []
    
    if not YOUTUBE_API_KEY:
        print("âš ï¸ [YouTube] API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. (GitHub Secrets í™•ì¸ í•„ìš”)")
        return []

    try:
        youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)
        
        # 24ì‹œê°„ ì´ë‚´ ì˜ìƒë§Œ
        search_date = datetime.utcnow() - timedelta(days=1) 
        published_after = search_date.strftime("%Y-%m-%dT%H:%M:%SZ")
        
        query = "|".join(YOUTUBE_SEARCH_TOPICS)

        # 1. ê²€ìƒ‰ (ì˜ìƒ ID ìˆ˜ì§‘)
        search_response = youtube.search().list(
            q=query, part="id", order="date",
            publishedAfter=published_after, type="video",
            maxResults=30 
        ).execute()

        video_ids = []
        seen_ids = set()
        for item in search_response.get("items", []):
            vid_id = item['id']['videoId']
            if vid_id not in seen_ids:
                video_ids.append(vid_id)
                seen_ids.add(vid_id)

        if not video_ids:
            print("   ğŸ’¨ ìµœê·¼ 24ì‹œê°„ ë‚´ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return []

        # 2. ìƒì„¸ ì •ë³´ ì¡°íšŒ (ì¡°íšŒìˆ˜ í™•ì¸ìš©)
        video_response = youtube.videos().list(
            id=','.join(video_ids), part='snippet,statistics'
        ).execute()

        items = video_response.get("items", [])
        # ì¡°íšŒìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        items.sort(key=lambda x: int(x['statistics'].get('viewCount', 0)), reverse=True)

        print(f"   ğŸ” 1ì°¨ ê²€ìƒ‰ëœ ì˜ìƒ {len(items)}ê°œ ë¶„ì„ ì¤‘...")

        for item in items:
            vid_id = item['id']
            stats = item['statistics']
            snippet = item['snippet']
            
            # [í•„í„°] ì¡°íšŒìˆ˜ 10íšŒ ë¯¸ë§Œ ì œì™¸
            view_count = int(stats.get('viewCount', 0))
            if view_count < 10:
                continue

            raw_title = snippet['title']
            channel = snippet['channelTitle']

            # [ì œëª© ì²˜ë¦¬] ë¸Œëœë“œ *ë³¼ë“œ*
            title_display = raw_title
            brand_detected = False
            for brand in TARGET_BRANDS:
                if brand in raw_title:
                    title_display = title_display.replace(brand, f"*{brand}*")
                    brand_detected = True
            
            # [ì˜ìƒ ì €ì¥] ë¸Œëœë“œ ì–¸ê¸‰ì´ ìˆê±°ë‚˜, ì¡°íšŒìˆ˜ê°€ ë†’ì€ ê´€ë ¨ ì˜ìƒ
            # (ì—¬ê¸°ì„œëŠ” ë¸Œëœë“œ ì–¸ê¸‰ ì—¬ë¶€ì™€ ìƒê´€ì—†ì´ ì£¼ì œê°€ ë§ê³  ì¡°íšŒìˆ˜ í†µê³¼í•˜ë©´ ì €ì¥í•˜ë˜, 
            # ë¸Œëœë“œê°€ ìˆìœ¼ë©´ ì œëª©ì— ë³¼ë“œì²˜ë¦¬ë¨)
            results.append({
                "ì‘ì„±ì¼": datetime.now().strftime("%Y-%m-%d") + " (New)",
                "í‚¤ì›Œë“œ": "ìœ íŠœë¸Œ(ì˜ìƒ)",
                "ì¹´í˜ëª…": f"[YouTube] {channel}",
                "ì œëª©": f"[ì˜ìƒ] {title_display} (ì¡°íšŒìˆ˜ {view_count}íšŒ)",
                "ë§í¬": f"https://www.youtube.com/watch?v={vid_id}",
                "ìˆ˜ì§‘ì‹œì ": datetime.now().strftime("%Y-%m-%d %H:%M")
            })

            # 3. ëŒ“ê¸€ ìˆ˜ì§‘
            try:
                comment_response = youtube.commentThreads().list(
                    videoId=vid_id, part="snippet", textFormat="plainText", maxResults=5
                ).execute()
                
                for c_item in comment_response.get("items", []):
                    comment = c_item['snippet']['topLevelComment']['snippet']
                    text = comment['textDisplay'].replace('\n', ' ').strip()
                    author = comment['authorDisplayName']

                    # ëŒ“ê¸€ ë‚´ìš© ë¸Œëœë“œ *ë³¼ë“œ* ì²˜ë¦¬
                    found_brand_in_comment = False
                    for brand in TARGET_BRANDS:
                        if brand in text:
                            text = text.replace(brand, f"*{brand}*")
                            found_brand_in_comment = True
                    
                    # ë¸Œëœë“œê°€ ì–¸ê¸‰ëœ ëŒ“ê¸€ë§Œ ì €ì¥ (ëŒ“ê¸€ì€ í•„í„°ë§ ê°•í™”)
                    if found_brand_in_comment:
                        if len(text) > 80: text = text[:80] + "..."
                        results.append({
                            "ì‘ì„±ì¼": datetime.now().strftime("%Y-%m-%d") + " (New)",
                            "í‚¤ì›Œë“œ": "ìœ íŠœë¸Œ(ëŒ“ê¸€)",
                            "ì¹´í˜ëª…": f"[YouTube] {author}",
                            "ì œëª©": f"ğŸ’¬ {text}",
                            "ë§í¬": f"https://www.youtube.com/watch?v={vid_id}",
                            "ìˆ˜ì§‘ì‹œì ": datetime.now().strftime("%Y-%m-%d %H:%M")
                        })
            except HttpError:
                pass 

    except Exception as e:
        print(f"âŒ [YouTube] ì—ëŸ¬ ë°œìƒ: {e}")
    
    print(f"   âœ… ìœ íŠœë¸Œ ë°ì´í„° {len(results)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
    return results

# ======================================================
# [ê¸°ëŠ¥ 2] ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ í•¨ìˆ˜ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
# ======================================================
def crawl_naver():
    print(f"\nğŸš€ [Naver] ê³ ì„±ëŠ¥ í¬ë¡¤ëŸ¬ ì‹œì‘")
    data_list = []

    options = webdriver.ChromeOptions()
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    options.add_argument("--headless") 
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    time.sleep(2)

    try:
        for keyword in NAVER_SEARCH_KEYWORDS:
            print(f"   ğŸ” '{keyword}' ê²€ìƒ‰ ì¤‘...")
            base_url = "https://search.naver.com/search.naver?ssc=tab.cafe.all&st=date&nso=so%3Add%2Cp%3Aall&query="
            driver.get(base_url + keyword)
            time.sleep(3) 

            # ì‹¬ì¸µ ê²€ìƒ‰ ë¡œì§
            if keyword in DEEP_SEARCH_KEYWORDS:
                print(f"      ğŸ‘‰ ì‹¬ì¸µ ê²€ìƒ‰ ì§„í–‰ (15 pg)")
                scroll_times = 15 
            else:
                scroll_times = 3 

            body = driver.find_element(By.TAG_NAME, "body")
            for i in range(scroll_times):
                body.send_keys(Keys.END)
                time.sleep(1.2)
            
            time.sleep(2)

            articles = driver.find_elements(By.CSS_SELECTOR, "li.bx")
            keyword_count = 0
            
            for article in articles:
                try:
                    # ì¹´í˜ëª…
                    cafe_name = ""
                    try: cafe_name = article.find_element(By.CSS_SELECTOR, "a.txt_name").text
                    except: 
                        try: cafe_name = article.find_element(By.CSS_SELECTOR, "a.name").text
                        except: pass

                    # í•„í„°
                    if not any(target in cafe_name for target in TARGET_CAFE_KEYWORDS): continue
                    if not any(x in article.text for x in ["ë¶„ ì „", "ì‹œê°„ ì „", "ë°©ê¸ˆ ì „"]): continue
                    
                    # ì œëª©/ë§í¬
                    try:
                        title_ele = article.find_element(By.CSS_SELECTOR, "a.title_link")
                        title = title_ele.text
                        link = title_ele.get_attribute("href")
                    except: continue

                    if any(bad_word in title for bad_word in EXCLUDE_WORDS): continue

                    data_list.append({
                        "ì‘ì„±ì¼": datetime.now().strftime("%Y-%m-%d") + " (New)",
                        "í‚¤ì›Œë“œ": keyword, 
                        "ì¹´í˜ëª…": cafe_name,
                        "ì œëª©": title,
                        "ë§í¬": link,
                        "ìˆ˜ì§‘ì‹œì ": datetime.now().strftime("%Y-%m-%d %H:%M")
                    })
                    keyword_count += 1
                except Exception: continue
            
            print(f"      âœ¨ ìˆ˜ì§‘: {keyword_count}ê±´")
            time.sleep(1)

    except Exception as e:
        print(f"ì—ëŸ¬ ë°œìƒ: {e}")
    finally:
        driver.quit()
    
    return data_list

# ======================================================
# [ê¸°ëŠ¥ 3] ë©”ì¸ ì‹¤í–‰ ë° ì €ì¥/ì—…ë¡œë“œ
# ======================================================
if __name__ == "__main__":
    # 1. ìˆ˜ì§‘
    naver_data = crawl_naver()
    youtube_data = crawl_youtube()
    
    # 2. í•©ì¹˜ê¸°
    all_data = naver_data + youtube_data
    
    # 3. ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥
    if all_data:
        df_new = pd.DataFrame(all_data)
        # ì»¬ëŸ¼ ìˆœì„œ ê°•ì œ ê³ ì •
        df_new = df_new[["ì‘ì„±ì¼", "í‚¤ì›Œë“œ", "ì¹´í˜ëª…", "ì œëª©", "ë§í¬", "ìˆ˜ì§‘ì‹œì "]]

        if os.path.exists(FILE_NAME):
            try:
                df_old = pd.read_csv(FILE_NAME)
                # êµ¬ë²„ì „ íŒŒì¼ í˜¸í™˜ì„± ì²´í¬
                if "í‚¤ì›Œë“œ" not in df_old.columns:
                     df_new.to_csv(FILE_NAME, mode='w', header=True, index=False, encoding="utf-8-sig")
                else:
                    existing_links = df_old['ë§í¬'].tolist()
                    df_final = df_new[~df_new['ë§í¬'].isin(existing_links)]
                    
                    if not df_final.empty:
                        df_final.to_csv(FILE_NAME, mode='a', header=False, index=False, encoding="utf-8-sig")
                        print(f"\nğŸ’¾ ë¡œì»¬ ì €ì¥ ì™„ë£Œ ({len(df_final)}ê±´ ì¶”ê°€)")
                        
                        # GitHub ì—…ë¡œë“œ (ë°ì´í„°ê°€ ì¶”ê°€ë˜ì—ˆì„ ë•Œë§Œ ìˆ˜í–‰)
                        print("\nğŸ™ GitHub ì—…ë¡œë“œ ì‹œì‘...")
                        subprocess.run(["git", "config", "--global", "user.name", "GitHub Action Bot"], check=False)
                        subprocess.run(["git", "config", "--global", "user.email", "actions@github.com"], check=False)
                        subprocess.run(["git", "add", FILE_NAME], check=True)
                        commit_msg = f"Update data: {datetime.now().strftime('%Y-%m-%d %H:%M')}"
                        try:
                            subprocess.run(["git", "commit", "-m", commit_msg], check=True)
                            subprocess.run(["git", "push"], check=True)
                            print("âœ… GitHub Push ì™„ë£Œ!")
                        except subprocess.CalledProcessError:
                             print("   -> ì»¤ë°‹í•  ë³€ê²½ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        print("\nğŸ‘Œ ìƒˆë¡œìš´ ê¸€ì´ ì—†ìŠµë‹ˆë‹¤ (ì „ì²´ ì¤‘ë³µ).")
            except Exception as e:
                print(f"íŒŒì¼ ì²˜ë¦¬ ì—ëŸ¬: {e}")
                # ì—ëŸ¬ ì‹œ ë®ì–´ì“°ê¸° ì•ˆì „ì¥ì¹˜
                df_new.to_csv(FILE_NAME, mode='w', header=True, index=False, encoding="utf-8-sig")
        else:
            # íŒŒì¼ì´ ì•„ì˜ˆ ì—†ì„ ë•Œ
            df_new.to_csv(FILE_NAME, mode='w', header=True, index=False, encoding="utf-8-sig")
            print(f"\nâœ… ì‹ ê·œ íŒŒì¼ ìƒì„± ì™„ë£Œ.")
            subprocess.run(["git", "add", FILE_NAME], check=True)
            subprocess.run(["git", "commit", "-m", "Init data"], check=True)
            subprocess.run(["git", "push"], check=True)
    else:
        print("\nğŸ’¤ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
